{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Reveal Strategy\n",
    "This commit reveal strategy aims to tackle unwanted weight copying behaviour bittensor. This notebook will guide you through how to run diagnostic in your SN to determine a correct parameter to set for your SN. \n",
    "\n",
    "## Background\n",
    "Validators are encouraged to do validation work to increase competitveness in a subnet. Through reaching consensus, validators would be rewarded with dividend for their contribution in the SN. Yet, some validators choose to reach consensus through direct copying weights that was produced by other validators which hurts the decentralization characteristic of bittensor.\n",
    "\n",
    "Commit reveal was designed such that there will be offset in time when the weights are generated by the validators and the concensus are calculated. Thus, the weights that weight copier set would always be at least `commit_reveal_weight_interval` later than the original weight. The idea is that `commit_reveal_weight_interval` should be long enough such that when the copier does the copy, there would be enough change in the network, so that the weight to copy would already be irrelevant.\n",
    "\n",
    "Here it illustrates the difference in the existing system VS the commit reveal system. \n",
    "\n",
    "For the ease of illustration, assume `conceal_period = floor(commit_reveal_weight_interval / 360)`, we also suggest users to set the `commit_reveal_weight_interval` to be a multiple of 360 blocks (a tempo).\n",
    "\n",
    "#### === Existing system ===\n",
    "| Epoch  (360 blocks = 1 tempo apart)    | Actor |Actions                                                                                         |\n",
    "|-----------|--------|-----------------------------------------------------------------------------------------|\n",
    "| n     | Validators |Does evaluation and set weight to the chain.                                          |\n",
    "| n     | Weight copier | Weights would be available on the chain, weight copier can copy weights available. |\n",
    "| n | Chain |Calculates consensus based on validator weights.                                           |\n",
    "| n | Weight copier|Consensus would be available on the chain, weight copier can copy consensus available.   |\n",
    "\n",
    "#### === Commit reveal system ===\n",
    "| Epoch (360 blocks = 1 tempo apart)              | Actor | Action                                                                                                                       |\n",
    "|--------------------|-----------------|---------------------------------------------------------------------------------------------------------|\n",
    "| n - `conceal_period` | Validators | Does evaluation and set hashed weight hash(hotkey, weight_old) to the chain.                                      |\n",
    "| n                  | Validators | Set weight to the chain that corresponds to hash(hotkey, weight_old).                                             |\n",
    "| n                  | Weight copier | Weights would be available on the chain, weight copier can copy weights available and set hash(hotkey, weight_old) to chain. |\n",
    "| n                  | Chain |Calculates consensus based on hashes received on n - `conceal_period`.                                               |\n",
    "| n                  | Weight copier | Consensus would be available on the chain, weight copier can copy consensus available and set them as hash(hotkey, weight_old) to chain. |\n",
    "\n",
    "* notice how when weight copier set weights, the weight it set is already `concel_period` apart from when the weight was generated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disanostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from multiprocessing import Pool\n",
    "import torch \n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure\n",
    "import bittensor as bt\n",
    "\n",
    "import pickle\n",
    "from experiment_setup import ExperimentSetup\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "setup = ExperimentSetup(\n",
    "    netuids = [4],\n",
    "    start_block = 4509000 - 100 * 360,\n",
    "    data_points = 100,\n",
    "    processes = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download metagraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_metagraphs import DownloadMetagraph\n",
    "DownloadMetagraph(setup = setup).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_copy_simulation import WeightCopySimulation\n",
    "WeightCopySimulation(setup = setup).run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting relative dividend rate\n",
    "\n",
    "With $D$ as dividend; $S$ as stake; $\\mathcal Z$ as the set of validators.\n",
    "We use the relative dividend rate of the copier $j$,\n",
    "$$G^j = \\frac{D^j/S^j}{\\underset{i \\in \\mathcal Z \\setminus \\{j\\}}{\\mathrm{median}} \\{D^i/S^i\\}}$$\n",
    "to measure the success of the commit-reveal approach. Here, validator dividend is normalized by the corresponding validator stake as dividend is linear in the amount of stake. Further, we use median as the baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_losts = {}\n",
    "yuma_results = {}\n",
    "\n",
    "for netuid in setup.netuids:\n",
    "    div_losts[netuid] = {}\n",
    "    yuma_results[netuid] = {}\n",
    "    \n",
    "    for conceal_period in setup.conceal_periods:\n",
    "        with open(f\"{setup.result_path}/yuma_result_netuid{netuid}_conceal{conceal_period}.pkl\", 'rb') as handle:\n",
    "            _yuma_results = pickle.load(handle)\n",
    "\n",
    "        dividend = [\n",
    "            (s[\"validator_reward_normalized\"] / s[\"stake\"]).tolist()\n",
    "            for idx, s in _yuma_results.items()\n",
    "        ]\n",
    "\n",
    "        dividend_df = pd.DataFrame(\n",
    "            dividend,\n",
    "            columns=[f\"v{i}\" for i in range(len(dividend[0]) - 1 )] + [\"v_bad\"],\n",
    "            index = _yuma_results.keys()\n",
    "        )\n",
    "        \n",
    "        div_last = dividend_df.iloc[-1]\n",
    "        div_lost = div_last[-1] / div_last[:-1].median()\n",
    "\n",
    "        div_losts[netuid][conceal_period] = div_lost\n",
    "        yuma_results[netuid][conceal_period] = _yuma_results\n",
    "\n",
    "div_losts = pd.DataFrame(div_losts, dtype='float64')# index as conceal periods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting similarity of weight across conceal periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = {}\n",
    "\n",
    "for netuid in setup.netuids:\n",
    "    similarities[netuid] = {}\n",
    "    \n",
    "    for conceal_period in setup.conceal_periods:\n",
    "\n",
    "        if conceal_period == 0:\n",
    "            similarities[netuid][conceal_period] = 1\n",
    "            continue\n",
    "\n",
    "        similarity = torch.load(f\"{setup.result_path}/similarity_netuid{netuid}_conceal{conceal_period}.pt\")\n",
    "        similarities[netuid][conceal_period] = similarity.mean().item()\n",
    "\n",
    "similarities = pd.DataFrame(similarities, dtype='float64')# index as conceal periods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the changes across conceal periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    div_losts,\n",
    "    labels={\n",
    "        \"value\": \"Relative dividend rate (G)\".title(),\n",
    "        \"index\": \"Conceal period (every 360 blocks)\",\n",
    "        \"variable\": 'Subnet'\n",
    "    },\n",
    "    title=\"Relative Dividend Rate Of Weight Copier\",\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    ")\n",
    "fig.add_hline(y=1, line_width=3, line_dash=\"dash\", line_color=\"red\", annotation_text = \"\")\n",
    "fig.update_layout(template='plotly_white')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "For the conceal period to be effective, you should set a conceal period large enough to produce enough lost in dividend for the weight copier.\n",
    "\n",
    "| Dividend gain (G) | Effect                                                                                                     |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| < 1               | Norminator lost the incentive to delegate to weight copier, weight copier earn less validator take.  |\n",
    "| < 0.82          | Weigh copier lost the incentive to copy weight.                                                      |\n",
    "\n",
    "If given a conceal period long enough (>15 hours) and the SN still fail to produce enough lost in dividend, it means that there is not enough churn and weight movement in the SN, so the existing weight copiy fix may not work for your SN. Depending on the situation, you may choose to increase competitiveness/ churn in your SN or just leave the weight copier as is. Cause when there is no churn in the SN, there would be no movement in consensus as well, so the weight copier would not be as beneficial. \n",
    "\n",
    "Note that when the conceal period was set too long, it would slow down the discovery of new miners, putting them at risk for deregistration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
